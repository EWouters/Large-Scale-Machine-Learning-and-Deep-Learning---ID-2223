\documentclass[a4paper]{article}

\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage{hyperref}


\title{Scalable Machine Learning - Review Questions 2}
\author{TheBrightSideOfLife: Giorgio Ruffa - Erik Wouters}
\date{09/11/2018}

\begin{document}

\maketitle

\section{Cross Validation}
%Use part of the training data as validation data and rotate which part.
%\url{https://en.wikipedia.org/wiki/Cross-validation_(statistics)}

Cross validation in machine learning is the technique of taking a part of the training data, removing it from the training data before training and using it after training to evaluate the performance of a model.

There are multiple types of cross validation. The holdout method is as described above. The results of this method can vary, depending on the particular selection of the training and validation sets. To reduce this dependency, $k$-fold cross validation can be used. In this case the training data is divided in $k$ subsets and the holdout method is repeated $k$ times, each subset $k$ serves as the validation data exactly once. When we increase $k$ to the number of samples in the training set $m$ we get leave-one-out cross validation.

%/url{https://www.cs.cmu.edu/~schneide/tut5/node42.html}

\section{Softmax}
The sigmoid function used in binary logistic regression is defined as following:
\begin{align*}
    P(\hat{y}=0 | \mathbf{w}^{T}, \mathbf{x}) &= \frac{e^{-\mathbf{w}^T \mathbf{x}}}{1 + e^{-\mathbf{w}^T \mathbf{x}}} 
    \\
    P(\hat{y}=1 | \mathbf{w}^{T}, \mathbf{x}) &= 1 - P(\hat{y}=0 | \mathbf{w}^{T}, \mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}}} 
\end{align*}

While the softmax function is defined as
\begin{align}
\sigma(\mathbf{w}^T_j\mathbf{x}) &= 
P(\hat{y} = j | \mathbf{x}, \mathbf{w}_j) =
\frac{e^{\mathbf{w}^T_j\mathbf{x}}}{\sum^k_{i=1}e^{\mathbf{w}^T_i\mathbf{x}}}
\end{align}
where
\begin{align}
\mathbf{w}^T_i &= [w_{0,i},w_{1,i},\dots,w_{n,i}]
\end{align}
with $n$ the number of input features; and
\begin{align*}
    j \in \{1, ... ,k\}
\end{align*}
which is an index representing each separate, and mutually exclusive, class while $k$ is a finite integer.

Note that for the softmax function we have that the sum of the probability outcomes for each class $j \in \{1,...,k\}$ is equal to one
\begin{align} \label{eq:normalization}
    \sum_{j=i}^{k} P(y=j | \mathbf{x}, \mathbf{w_j}) =
    \frac{\sum^k_{j=1}e^{\mathbf{w}^T_j\mathbf{x}}}{\sum^k_{i=1}e^{\mathbf{w}^T_i\mathbf{x}}} =
    1
\end{align}

When we have two classes ($k = 2$) we get
\begin{align*}
\sigma(\mathbf{w}^T_j\mathbf{x}) &= \frac{e^{\mathbf{w}^T_j\mathbf{x}}}{\sum^2_{i=1}e^{\mathbf{w}^T_i\mathbf{x}}} \\
&= \frac{e^{\mathbf{w}^T_j\mathbf{x}}}{e^{\mathbf{w}^T_1\mathbf{x}} + e^{\mathbf{w}^T_2\mathbf{x}}} \\
\end{align*}

For $j = 1$

\begin{align*}
\sigma(\mathbf{w}^T_1\mathbf{x}) &= \frac{e^{\mathbf{w}^T_1\mathbf{x}}}{e^{\mathbf{w}^T_1\mathbf{x}} + e^{\mathbf{w}^T_2\mathbf{x}}} \\
\end{align*}

We can now take divide the numerator and denominator by $e^{\mathbf{w}^T_1\mathbf{x}}$ and we get

\begin{align*}
\sigma(\mathbf{w}^T_1\mathbf{x}) = \frac{1}{1 + e^{(\mathbf{w}^T_2-\mathbf{w}^T_1)\mathbf{x}}} 
= \frac{1}{1 + e^{- (\mathbf{w}^T_1-\mathbf{w}^T_2)\mathbf{x}}}
\end{align*}

We can calculate $\sigma(\mathbf{w}^T_2\mathbf{x})$ using the normalization condition of equation \ref{eq:normalization}:

\begin{align*}
\sigma(\mathbf{w}^T_2\mathbf{x}) = 1 - P(\hat{y} = 1 | \mathbf{x},\mathbf{w}_1) = 1 - \sigma(\mathbf{w}^T_1\mathbf{x}) = 1 - \frac{1}{1 + e^{- (\mathbf{w}^T_1-\mathbf{w}^T_2)\mathbf{x}}} = \frac{e^{- (\mathbf{w}^T_1-\mathbf{w}^T_2)\mathbf{x}}}{1 + e^{- (\mathbf{w}^T_1-\mathbf{w}^T_2)\mathbf{x}}}
\end{align*}
If we define $\mathbf{w} = \mathbf{w_1} - \mathbf{w_2}$ we obtain the same form of the sigmoid function, hence the equivalence.

Less formally, we can say that the equivalence of the two forms builds upon the fact that the classes are mutually exclusive, which implies that the sum of the probabilities of observing a class must be equal to one.



% \url{https://stats.stackexchange.com/questions/237893/should-k-2-softmax-regression-and-logistic-regression-give-the-same-results}
% Let $Y \in \{0,1\}$ be the outcome variable and let $X$ denote a single predictor. Without the bias term, the softmax functions can be written as:

% $P(Y=0|X=x) = \dfrac{e^{x\beta_0}}{e^{x \beta_0}+e^{x\beta_1}} = \dfrac{1}{1+e^{x(\beta_1-\beta_0)}}$

% $P(Y=1|X=x) = \dfrac{e^{x\beta_1}}{e^{x \beta_0}+e^{x\beta_1}} = \dfrac{e^{x(\beta_1-\beta_0)}}{1+e^{x(\beta_1-\beta_0)}}$

% Hence, the probabilities only depend on the difference $\beta_1 - \beta_0$. Choosing $\beta_0=8$ and $\beta_1=10$, for example, will give you the same value of the logistic loss function as $\beta_0=108$ and $\beta_1=110$. Both parameters are therefore not individually identifiable.


% \url{https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier}
% In the two-class logistic regression, the predicted probablies are as follows, using the sigmoid function:

% \begin{align} \Pr(Y_i=0) &= \frac{e^{-\boldsymbol\beta_ \cdot \mathbf{X}_i}} {1 +e^{-\boldsymbol\beta_0 \cdot \mathbf{X}_i}} \, \\ \Pr(Y_i=1) &= 1 - \Pr(Y_i=0) = \frac{1} {1 +e^{-\boldsymbol\beta_ \cdot \mathbf{X}_i}} \end{align}

% In the multiclass logistic regression, with $K$ classes, the predicted probabilities are as follows, using the softmax function:

% \begin{align} \Pr(Y_i=k) &= \frac{e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}} {~\sum_{0 \leq c \leq K}^{}{e^{\boldsymbol\beta_c \cdot \mathbf{X}_i}}} \, \\ \end{align}

% One can observe that the softmax function is an extension of the sigmoid function to the multiclass case, as explained below. Let's look at the multiclass logistic regression, with $K=2$ classes:

% \begin{align} \Pr(Y_i=0) &= \frac{e^{\boldsymbol\beta_0 \cdot \mathbf{X}_i}} {~\sum_{0 \leq c \leq K}^{}{e^{\boldsymbol\beta_c \cdot \mathbf{X}_i}}} = \frac{e^{\boldsymbol\beta_0 \cdot \mathbf{X}_i}}{e^{\boldsymbol\beta_0 \cdot \mathbf{X}_i} + e^{\boldsymbol\beta_1 \cdot \mathbf{X}_i}} = \frac{e^{(\boldsymbol\beta_0 - \boldsymbol\beta_1) \cdot \mathbf{X}_i}}{e^{(\boldsymbol\beta_0 - \boldsymbol\beta_1) \cdot \mathbf{X}_i} + 1} = \frac{e^{-\boldsymbol\beta_ \cdot \mathbf{X}_i}} {1 +e^{-\boldsymbol\beta \cdot \mathbf{X}_i}} \\ \, \\ \Pr(Y_i=1) &= \frac{e^{\boldsymbol\beta_1 \cdot \mathbf{X}_i}} {~\sum_{0 \leq c \leq K}^{}{e^{\boldsymbol\beta_c \cdot \mathbf{X}_i}}} = \frac{e^{\boldsymbol\beta_1 \cdot \mathbf{X}_i}}{e^{\boldsymbol\beta_0 \cdot \mathbf{X}_i} + e^{\boldsymbol\beta_1 \cdot \mathbf{X}_i}} = \frac{1}{e^{(\boldsymbol\beta_0-\boldsymbol\beta_1) \cdot \mathbf{X}_i} + 1} = \frac{1} {1 +e^{-\boldsymbol\beta_ \cdot \mathbf{X}_i}} \, \\ \end{align}

% with $\boldsymbol\beta = - (\boldsymbol\beta_0 - \boldsymbol\beta_1)$. We see that we obtain the same probabilities as in the two-class logistic regression using the sigmoid function. Wikipedia expands a bit more on that.



\section{Binomial Logistic Regression}
%\url{https://www.coursera.org/lecture/neural-networks-deep-learning/logistic-regression-cost-function-yWaRd}

The basic cost function is based on the Mean Squared Error (MSE):
\begin{align}
\operatorname{cost}(\hat{y}^{(i)},y^{(i)}) &= (\hat{y}^{(i)} - y^{(i)})^2
\end{align}
where
\begin{align}
\hat{y}^{(i)} &= \frac{1}{1 + e^{-\mathbf{w}^T_i\mathbf{x}}}
\end{align}
This function is not convex however, and can get stuck in a local minimum when we use a gradient decent algorithm to find the weights that minimize this function and thus make $\hat{y}^{(i)}$ approximate $y^{(i)}$.

We need to have a function that is close to $0$ if the predicted value $\hat{y}^{(i)}$ will be close to the true value $y^{(i)}$, and large if the predicted value $\hat{y}^{(i)}$ will be far from the true value $y^{(i)}$. The $-\log(\hat{y}^{(i)})$ function has exactly that property if $y^{(i)}=1$ and $-\log(1-\hat{y}^{(i)})$ has that property if $y^{(i)}=0$. Moreover this function gives us a nice convex surface which we can use to efficiently optimize the weights.

%and $y^{(i)} \in {0,1}$, so we have two cases, $y^{(i)}=0$ and $y^{(i)}=1$. For $y^{(i)}=0$ we get
%\begin{align}
%\operatorname{cost}(\hat{y}^{(i)},y^{(i)}) &= (\frac{1}{1 + e^{-\mathbf{w}^T_i\mathbf{x}}})^2
%\end{align}
%We are minimizing the this cost function. %And because the exponential function is a convex function.

\section{Logistic regression cost, cross-entropy, log-likelihood}
The cross-entropy is a measure of the similarity of two probability distributions over a set of common events. In the case of machine learning, the two compared distributions are the ground truth versus the probability obtained by the model. So we can see the cross entropy as being a more general definition of a cost function. The goal of the model training is, in fact, minimizing the difference between the observed ground truth and the probability distribution obtained from the trained model. In the case of logistic regression, by considering that the number of classes is two and that the sum of the probabilities for both the real distribution and the model distribution must be one, the cross-entropy is equivalent to the cost function by a constant factor of $\frac{1}{m}$. Hence minimizing the cross-entropy is the same as minimizing the cost function.

As for the likelihood, by training a model we are trying to maximize the likelihood of the data under a probabilistic model given by a set of parameters ( or weights ). Given the numerical instability associated to the calculation of products with very low values (probabilities), instead of maximizing the product is it better to maximize the logarithm of the product (which is a sum). By minimizing the negative of the logarithm of the likelihood, for the case of a binary classifier, we obtain the cross-entropy.

\section{ROC curve}
The Receiver Operating Characteristic (ROC) curve is the True Positive Rate (TPR) as a function of the False Positive Rate (FPR). TPR is the probability that $\hat{y}^{(i)}=1$ given $y^{(i)}=1$, and FPR is the probability that $\hat{y}^{(i)}=1$ given $y^{(i)}=0$.

For a perfect classifier the TPR is $1$ and the FPR is $0$. For a random classifier both are $0.5$. We can use the ROC curve to measure the performance of the classifier. Maximizing the area under the ROC curve yields the best performance for the classifier.

\end{document}